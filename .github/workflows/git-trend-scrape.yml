name: Scrape GitHub Trending and Upsert to Supabase

on:
  schedule:
    - cron: '0 * * * *' # Runs every hour
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.x'

    - name: Install dependencies
      run: |
        pip install requests beautifulsoup4 supabase
        pip freeze  # Debugging to show installed packages
    
    - name: Scrape GitHub Trending Repositories and Upload to Supabase
      run: |
        python - <<EOF
        from supabase import create_client, Client
        import requests
        from bs4 import BeautifulSoup
        from datetime import datetime
        import os

        # Initialize Supabase client
        SUPABASE_URL = os.getenv('SUPABASE_URL')
        SUPABASE_KEY = os.getenv('SUPABASE_KEY')
        supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

        # GitHub Trending URL
        URL = 'https://github.com/trending'

        def scrape_trending_repos():
            print("Fetching GitHub trending repositories...")
            response = requests.get(URL)
            
            # Print the status code
            print(f"GitHub response status: {response.status_code}")
            
            if response.status_code != 200:
                print(f"Failed to retrieve data: {response.status_code}")
                return None

            print("Parsing the response...")

            # Print the full raw HTML content for debugging purposes
            print("Raw HTML content:")
            print(response.text[:5000])  # Print the first 5000 characters of HTML to inspect structure

            soup = BeautifulSoup(response.content, 'html.parser')
            repo_list = []
            repos = soup.find_all('article', class_='Box-row')

            print(f"Found {len(repos)} repositories on the page.")  # Print how many repositories were found
            
            for repo in repos:
                repo_name_tag = repo.find('h1', class_='h3 lh-condensed')
                repo_name = repo_name_tag.a.get_text(strip=True).replace('\n', ' ') if repo_name_tag and repo_name_tag.a else 'No name found'
                repo_link = 'https://github.com' + repo_name_tag.a['href'] if repo_name_tag and repo_name_tag.a else None
                stars_tag = repo.find('a', class_='Link--muted d-inline-block mr-3')
                stars = stars_tag.get_text(strip=True) if stars_tag else 'No stars found'
                description_tag = repo.p
                description = description_tag.get_text(strip=True) if description_tag else 'No description'

                # Only append if we have a valid link
                if repo_link:
                    repo_list.append({
                        'name': repo_name,
                        'link': repo_link,
                        'stars': stars,
                        'description': description,
                        'scraped_at': datetime.utcnow().isoformat()
                    })

            return repo_list

        def save_to_supabase(repos):
            print(f"Upserting {len(repos)} repositories to Supabase...")
            for repo in repos:
                data = {
                    'name': repo['name'],
                    'link': repo['link'],
                    'stars': repo['stars'],
                    'description': repo['description'],
                    'scraped_at': datetime.utcnow().isoformat()
                }
                try:
                    # Upsert data based on the repository link (unique field)
                    response = supabase.table('github_trending_repos').insert(data, upsert=True).execute()
                    print(f"Upserted: {repo['name']}, Response: {response}")
                except Exception as e:
                    print(f"Error upserting {repo['name']}: {e}")

        if __name__ == "__main__":
            trending_repos = scrape_trending_repos()
            if trending_repos:
                save_to_supabase(trending_repos)
                print(f"Successfully scraped and uploaded {len(trending_repos)} repositories.")
            else:
                print("No repositories scraped.")
        EOF
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}

    - name: Post job cleanup
      run: |
        echo "Post job cleanup."
