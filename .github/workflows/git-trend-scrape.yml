name: Scrape GitHub Trending Repositories

on:
  push:
  workflow_dispatch:
  schedule:
    - cron: '0 9 * * *' # Run daily at 09:00 AM UTC

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
    - name: Check out this repo
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.x'

    - name: Install Python dependencies
      run: |
        pip install requests beautifulsoup4 supabase

    - name: Scrape GitHub Trending Repositories and Upload to Supabase
      run: |
        python - <<EOF
        import requests
        from bs4 import BeautifulSoup
        import json
        from datetime import datetime
        from supabase import create_client, Client
        import os

        # GitHub Trending URL
        URL = 'https://github.com/trending'

        # Function to scrape trending repositories from GitHub
        def scrape_trending_repos():
            response = requests.get(URL)
            if response.status_code != 200:
                print(f"Failed to retrieve data: {response.status_code}")
                return None

            soup = BeautifulSoup(response.content, 'html.parser')
            repo_list = []
            repos = soup.find_all('article', class_='Box-row')

            for repo in repos:
                repo_name = repo.h1.get_text(strip=True).replace('\n', ' ') if repo.h1 else 'No name found'
                repo_link = 'https://github.com' + repo.h1.a['href'] if repo.h1 and repo.h1.a else 'No link found'
                stars = repo.find('a', class_='Link--muted d-inline-block mr-3')
                stars = stars.get_text(strip=True) if stars else 'No stars found'
                description_tag = repo.p
                description = description_tag.get_text(strip=True) if description_tag else 'No description'

                repo_list.append({
                    'name': repo_name,
                    'link': repo_link,
                    'stars': stars,
                    'description': description,
                    'scraped_at': datetime.utcnow().isoformat()
                })

            return repo_list

        # Function to upload scraped data to Supabase
        def upload_to_supabase(data):
            supabase_url = os.getenv("SUPABASE_URL")
            supabase_key = os.getenv("SUPABASE_KEY")
            if not supabase_url or not supabase_key:
                print("Supabase credentials missing.")
                return
            
            try:
                supabase: Client = create_client(supabase_url, supabase_key)
                print("Connected to Supabase.")

                response = supabase.table("github_trending_repos").insert(data).execute()
                if response.status_code != 201:
                    print(f"Failed to insert data: {response.status_code}")
                else:
                    print(f"Successfully inserted {len(data)} rows into Supabase.")
            except Exception as e:
                print(f"Error uploading data to Supabase: {str(e)}")

        # Main function to scrape and upload data
        if __name__ == "__main__":
            trending_repos = scrape_trending_repos()
            if trending_repos:
                print(f"Scraped {len(trending_repos)} trending repositories.")
                upload_to_supabase(trending_repos)
            else:
                print("No repositories scraped.")
        EOF

    - name: Check if there are changes
      run: git diff --quiet || echo "Changes detected"

    - name: Commit and push if the data has changed
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        git config user.name "GitHub Action"
        git config user.email "actions@users.noreply.github.com"
        git add github_trending_repos.json
        timestamp=$(date -u)
        git commit -m "Latest GitHub Trending Repositories: ${timestamp}" || echo "No changes to commit"
        git push --force
